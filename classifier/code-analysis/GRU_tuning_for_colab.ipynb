{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVil-Ts1LkyI"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kC0r8FJELkyM"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "U63gqVNILkyO",
    "outputId": "b4172e75-918e-419f-e079-06fa57ec5663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    " from google.colab import drive\n",
    " drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "qFPkfKQFLkyQ",
    "outputId": "fde185a3-345c-4281-e4ad-1ce4eb5e0b5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 01.model\n",
      " amidaTest2.py\n",
      " bert-master.zip\n",
      " bert_x.npy\n",
      " cabocha-0.69.tar.bz2\n",
      "'Colab Notebooks'\n",
      " C言語課題.gsite\n",
      " export\n",
      " fasttext.bin\n",
      " Japanese_L-12_H-768_A-12_E-30_BPE_transformers.zip\n",
      " ja.zip\n",
      " LBa.csv\n",
      " LBa.gsheet\n",
      " modified.pdf\n",
      " nmf.pdf\n",
      " Saitaihou.pdf\n",
      " train-val_pre.tsv\n",
      " train-val-pre.tsv\n",
      " train-val-small.tsv\n",
      " train-val-wakati-juman2.tsv\n",
      " train-val-wakati-juman-nva.tsv\n",
      " train-val-wakati-juman.tsv\n",
      " train-val-wakati-nva.tsv\n",
      " train-val-wakati.tsv\n",
      " vector_data\n",
      " 六義園.gdoc\n",
      " 無題のプレゼンテーション.gslides\n",
      " 試作\n"
     ]
    }
   ],
   "source": [
    "!ls ./drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6c0bBqw4OGBK"
   },
   "outputs": [],
   "source": [
    "%cp ./drive/My\\ Drive/train-val_pre.tsv ./\n",
    "%cp ./drive/My\\ Drive/fasttext.bin ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THUO98lVQj0k"
   },
   "outputs": [],
   "source": [
    "%mkdir model\n",
    "%mkdir model/tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "V8hsXr5iLkyR",
    "outputId": "3a58586d-6f97-43d3-d753-cf813912db63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os,sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from sklearn.model_selection import learning_curve\n",
    "from scipy import sparse\n",
    "from scipy.sparse import load_npz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import load_npz\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import SpatialDropout1D, Bidirectional\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.advanced_activations import ReLU, PReLU\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class Util:\n",
    "    @classmethod\n",
    "    def dump(cls, value, path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        joblib.dump(value, path, compress=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return joblib.load(path)\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.general_logger = logging.getLogger('general')\n",
    "        self.result_logger = logging.getLogger('result')\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        file_general_handler = logging.FileHandler('./model/general.log')\n",
    "        file_result_handler = logging.FileHandler('./model/result.log')\n",
    "        if len(self.general_logger.handlers) == 0:\n",
    "            self.general_logger.addHandler(stream_handler)\n",
    "            self.general_logger.addHandler(file_general_handler)\n",
    "            self.general_logger.setLevel(logging.INFO)\n",
    "            self.result_logger.addHandler(stream_handler)\n",
    "            self.result_logger.addHandler(file_result_handler)\n",
    "            self.result_logger.setLevel(logging.INFO)\n",
    "\n",
    "    def info(self, message):\n",
    "        # 時刻をつけてコンソールとログに出力\n",
    "        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n",
    "\n",
    "    def result(self, message):\n",
    "        self.result_logger.info(message)\n",
    "\n",
    "    def result_ltsv(self, dic):\n",
    "        self.result(self.to_ltsv(dic))\n",
    "\n",
    "    def result_scores(self, run_name, scores):\n",
    "        # 計算結果をコンソールと計算結果用ログに出力\n",
    "        dic = dict()\n",
    "        dic['name'] = run_name\n",
    "        dic['score'] = np.mean(scores)\n",
    "        for i, score in enumerate(scores):\n",
    "            dic[f'score{i}'] = score\n",
    "        self.result_ltsv(dic)\n",
    "\n",
    "    def now_string(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    def to_ltsv(self, dic):\n",
    "        return '\\t'.join(['{}:{}'.format(key, value) for key, value in dic.items()])\n",
    "\n",
    "def load_x_train(features, sparse=False):\n",
    "    if features == \"bow\":\n",
    "        matrix =load_npz('../vec/bow_train_x.npz').astype('float64')\n",
    "    elif features == \"n-gram\":\n",
    "        matrix = load_npz('../vec/n-gram_x.npz').astype('float64')\n",
    "    elif features == \"tf-idf\":\n",
    "        matrix = load_npz('../vec/tf-idf_x.npz').astype('float64')\n",
    "    elif features == \"n-gram-tf-idf\":\n",
    "        matrix = load_npz('../vec/n-gram-tf-idf_x.npz').astype('float64')\n",
    "    elif features == \"word2vec_mean\":\n",
    "        matrix = np.load('../vec/word2vec_pre_x_mean.npy', allow_pickle = True)\n",
    "    elif features == \"word2vec_max\":\n",
    "        matrix = np.load('../vec/word2vec_pre_x_max.npy', allow_pickle = True)\n",
    "    elif features == \"word2vec_concat\":\n",
    "        l = np.load('../vec/word2vec_pre_x_mean.npy', allow_pickle = True)\n",
    "        r = np.load('../vec/word2vec_pre_x_max.npy', allow_pickle = True)\n",
    "        matrix = np.hstack((l, r))\n",
    "    elif features == \"word2vec_hier\":\n",
    "        matrix = np.load('../vec/word2vec_pre_x_hier.npy', allow_pickle = True)\n",
    "    elif features == \"fasttext_mean\":\n",
    "        matrix = np.load('../vec/fasttext_x_mean.npy', allow_pickle = True)\n",
    "    elif features == \"fasttext_max\":\n",
    "        matrix = np.load('../vec/fasttext_x_max.npy', allow_pickle = True)\n",
    "    elif features == \"fasttext_concat\":\n",
    "        l = np.load('../vec/fasttext_x_mean.npy', allow_pickle = True)\n",
    "        r = np.load('../vec/fasttext_x_max.npy', allow_pickle = True)\n",
    "        matrix = np.hstack((l, r))\n",
    "    elif features == \"fasttext_hier\":\n",
    "        matrix = np.load('../vec/fasttext_x_hier.npy', allow_pickle = True)\n",
    "    elif features == \"doc2vec\":\n",
    "        matrix = np.load('../vec/doc2vec_pre_x.npy', allow_pickle=True)\n",
    "    elif features == \"raw_text\":\n",
    "        df = pd.read_table(\"./train-val_pre.tsv\", index_col=0)\n",
    "        matrix = np.array(df[\"text\"], dtype=str)\n",
    "    return matrix\n",
    "\n",
    "def load_y_train(features):\n",
    "    if features == \"bow_nva\":\n",
    "        return np.load('../vec/bow_train_y_nva.npy', allow_pickle = True)\n",
    "    elif features == \"raw_text\":\n",
    "        df = pd.read_table(\"./train-val_pre.tsv\", index_col=0)\n",
    "        return np.array(df[\"label\"], dtype=int)\n",
    "    else:\n",
    "        return np.load('../vec/y_full.npy', allow_pickle=True).astype('float64')\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "class Model(metaclass=ABCMeta):\n",
    "    def __init__(self, run_fold_name: str, params: dict) -> None:\n",
    "        \"\"\"コンストラクタ\n",
    "        :param run_fold_name: ランの名前とfoldの番号を組み合わせた名前\n",
    "        :param params: ハイパーパラメータ\n",
    "        \"\"\"\n",
    "        self.run_fold_name = run_fold_name\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, tr_x: np.array, tr_y: np.array,\n",
    "              va_x: Optional[np.array] = None,\n",
    "              va_y: Optional[np.array] = None) -> None:\n",
    "        \"\"\"モデルの学習を行い、学習済のモデルを保存する\n",
    "        :param tr_x: 学習データの特徴量\n",
    "        :param tr_y: 学習データの目的変数\n",
    "        :param va_x: バリデーションデータの特徴量\n",
    "        :param va_y: バリデーションデータの目的変数\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, te_x: np.array) -> np.array:\n",
    "        \"\"\"学習済のモデルでの予測値を返す\n",
    "        :param te_x: バリデーションデータやテストデータの特徴量\n",
    "        :return: 予測値\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, te_x: np.array, te_y: np.array) -> float:\n",
    "        \"\"\"学習済のモデルでのスコア値を返す\n",
    "        :te_x: np.array\n",
    "        :te_y: np.array\n",
    "        :return: 予測値\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model(self, feature: str) -> None:\n",
    "        \"\"\"モデルの保存を行う\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self, feature: str) -> None:\n",
    "        \"\"\"モデルの読み込みを行う\"\"\"\n",
    "        pass\n",
    "\n",
    "# tensorflowの警告抑制\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "class ModelLSTM(Model):\n",
    "    def __init__(self, run_fold_name, **params):\n",
    "        super().__init__(run_fold_name, params)\n",
    "\n",
    "    def train(self, tr_x, tr_y, va_x=None, va_y=None):\n",
    "        \"\"\" \n",
    "            tr_x : List[str] (example.) [ \"I am happy\", \"hello\" ]\n",
    "            tr_y : List[label]\n",
    "            embedding_model : gensim.models.KeyedVectors Object\n",
    "        \"\"\"\n",
    "        # scaling\n",
    "        validation = va_x is not None\n",
    "\n",
    "        # パラメータ\n",
    "        nb_classes = 2\n",
    "        embedding_dropout = self.params['embedding_dropout']\n",
    "        lstm_dropout = self.params['lstm_dropout']\n",
    "        lstm_recurrent_dropout = self.params['recurrent_dropout']\n",
    "        hidden_layers = int(self.params['hidden_layers'])\n",
    "        hidden_units = int(self.params['hidden_units'])\n",
    "        hidden_activation = self.params['hidden_activation']\n",
    "        hidden_dropout = self.params['hidden_dropout']\n",
    "        batch_norm = self.params['batch_norm']\n",
    "        optimizer_type = self.params['optimizer']['type']\n",
    "        optimizer_lr = self.params['optimizer']['lr']\n",
    "        batch_size = int(self.params['batch_size'])\n",
    "        nb_epoch = int(self.params['nb_epoch'])\n",
    "        embedding_model = self.params['embedding_model']\n",
    "        bidirectional = self.params['Bidirectional']\n",
    "        use_pre_embedding = not (embedding_model is None)\n",
    "\n",
    "        # using keras tokenizer here\n",
    "        self.token = Tokenizer(num_words=None)\n",
    "        self.max_len = 100\n",
    "        if validation:\n",
    "            self.token.fit_on_texts(list(tr_x) + list(va_x))\n",
    "        else:\n",
    "            self.token.fit_on_texts(list(tr_x))\n",
    "\n",
    "        xtrain_seq = self.token.texts_to_sequences(tr_x)\n",
    "        tr_x = pad_sequences(xtrain_seq, maxlen=self.max_len)\n",
    "        #tr_y = np_utils.to_categorical(tr_y, num_classes=nb_classes)\n",
    "\n",
    "        if validation:\n",
    "            xvalid_seq = self.token.texts_to_sequences(va_x)\n",
    "            va_x = pad_sequences(xvalid_seq, maxlen=self.max_len)\n",
    "            #va_y = np_utils.to_categorical(va_y, num_classes=nb_classes)\n",
    "\n",
    "        word_index = self.token.word_index\n",
    "\n",
    "        if use_pre_embedding:\n",
    "            # create an embedding matrix\n",
    "            vector_dim = embedding_model.vector_size\n",
    "            embedding_matrix = np.zeros((len(word_index) + 1, vector_dim))\n",
    "            for word, i in tqdm(word_index.items()):\n",
    "                embedding_vector = embedding_model.wv[word]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "        self.model = Sequential()\n",
    "        # input layer\n",
    "        if use_pre_embedding:\n",
    "            self.model.add(Embedding(\n",
    "                    input_dim=len(word_index) + 1, \n",
    "                    output_dim=vector_dim,\n",
    "                    input_length=self.max_len,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False))\n",
    "        else:\n",
    "            self.model.add(Embedding(input_dim=len(word_index) + 1, \n",
    "                    output_dim=300,\n",
    "                    input_length=self.max_len))\n",
    "\n",
    "        self.model.add(SpatialDropout1D(embedding_dropout))\n",
    "        if bidirectional:\n",
    "            self.model.add(Bidirectional(LSTM(100, dropout=lstm_dropout, recurrent_dropout=lstm_recurrent_dropout)))\n",
    "        else:\n",
    "            self.model.add(LSTM(100, dropout=lstm_dropout, recurrent_dropout=lstm_recurrent_dropout))\n",
    "        # 中間層\n",
    "        for i in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_units))\n",
    "            if batch_norm == 'before_act':\n",
    "                self.model.add(BatchNormalization())\n",
    "            if hidden_activation == 'prelu':\n",
    "                self.model.add(PReLU())\n",
    "            elif hidden_activation == 'relu':\n",
    "                self.model.add(ReLU())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.model.add(Dropout(hidden_dropout))\n",
    "\n",
    "        # 出力層\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # オプティマイザ\n",
    "        if optimizer_type == 'sgd':\n",
    "            optimizer = SGD(lr=optimizer_lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        elif optimizer_type == 'adam':\n",
    "            optimizer = Adam(lr=optimizer_lr, beta_1=0.9, beta_2=0.999, decay=0.)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # 目的関数、評価指標などの設定\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # エポック数、アーリーストッピング\n",
    "        # あまりepochを大きくすると、小さい学習率のときに終わらないことがあるので注意\n",
    "        patience = 12\n",
    "        # 学習の実行\n",
    "        if validation:\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience,\n",
    "                                            verbose=2, restore_best_weights=True)\n",
    "            history = self.model.fit(tr_x, tr_y, epochs=nb_epoch, batch_size=batch_size, verbose=2,\n",
    "                                validation_data=(va_x, va_y), callbacks=[early_stopping])\n",
    "        else:\n",
    "            history = self.model.fit(tr_x, tr_y, nb_epoch=nb_epoch, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    def predict(self, te_x):\n",
    "        xtest_seq = self.token.texts_to_sequences(te_x)\n",
    "        te_x = pad_sequences(xtest_seq, maxlen=self.max_len)\n",
    "        y_pred = self.model.predict(te_x)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, te_x, te_y):\n",
    "        y_pred = self.predict(te_x)\n",
    "        y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "        return accuracy_score(te_y, y_pred)\n",
    "\n",
    "    def save_model(self, feature):\n",
    "        model_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}.h5')\n",
    "        scaler_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}-scaler.pkl')\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        self.model.save(model_path)\n",
    "        Util.dump(self.scaler, scaler_path)\n",
    "\n",
    "    def load_model(self, feature):\n",
    "        model_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}.h5')\n",
    "        scaler_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}-scaler.pkl')\n",
    "        self.model = load_model(model_path)\n",
    "        self.scaler = Util.load(scaler_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "id": "-tl-nvEcLkyT",
    "outputId": "47255995-301e-45a9-c0ee-6c192a8ad3aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "  0%|          | 0/100 [00:00<?, ?it/s, best loss: ?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/181555 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:248: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "\n",
      "  9%|9         | 16426/181555 [00:00<00:01, 164255.85it/s]\n",
      "\u001b[A\n",
      " 18%|#7        | 32279/181555 [00:00<00:00, 162494.22it/s]\n",
      "\u001b[A\n",
      " 26%|##6       | 47766/181555 [00:00<00:00, 160128.15it/s]\n",
      "\u001b[A\n",
      " 36%|###5      | 64756/181555 [00:00<00:00, 162939.54it/s]\n",
      "\u001b[A\n",
      " 47%|####6     | 85238/181555 [00:00<00:00, 173587.76it/s]\n",
      "\u001b[A\n",
      " 58%|#####8    | 105927/181555 [00:00<00:00, 182393.46it/s]\n",
      "\u001b[A\n",
      " 70%|######9   | 126369/181555 [00:00<00:00, 188485.29it/s]\n",
      "\u001b[A\n",
      " 81%|########  | 147002/181555 [00:00<00:00, 193505.77it/s]\n",
      "\u001b[A\n",
      " 92%|#########2| 167141/181555 [00:00<00:00, 195802.37it/s]\n",
      "\u001b[A\n",
      "100%|##########| 181555/181555 [00:01<00:00, 181175.31it/s]\n",
      "\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41666 samples, validate on 8334 samples\n",
      "Epoch 1/2\n",
      " - 224s - loss: 0.7234 - acc: 0.5438 - val_loss: 0.6566 - val_acc: 0.6122\n",
      "\n",
      "Epoch 2/2\n",
      "  0%|          | 0/100 [04:00<?, ?it/s, best loss: ?]"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "from gensim.models import KeyedVectors, FastText\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "def objective(params):\n",
    "    global param\n",
    "    param.update(params)\n",
    "    model = ModelLSTM(\"LSTM\", **param)\n",
    "    print(\"TRAIN\")\n",
    "    model.train(tr_x, tr_y, va_x, va_y)\n",
    "    print(\"PREDICT\")\n",
    "    va_pred = model.predict(va_x)\n",
    "    score = log_loss(va_y, va_pred)\n",
    "    print(f'params: {params}, logloss: {score:.4f}')\n",
    "    # 情報を記録しておく\n",
    "    history.append((params, score))\n",
    "\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 基本となるパラメータ\n",
    "    param = {\n",
    "        'embedding_dropout' : 0.3,\n",
    "        'lstm_dropout' : 0.3,\n",
    "        'recurrent_dropout' : 0.3,\n",
    "        'hidden_layers': 3,\n",
    "        'hidden_units': 64,\n",
    "        'hidden_activation': 'relu',\n",
    "        'hidden_dropout': 0.8,\n",
    "        'batch_norm': 'before_act',\n",
    "        'optimizer': {'type': 'adam', 'lr': 0.001},\n",
    "        'batch_size': 100,\n",
    "        'nb_epoch' : 2,\n",
    "        'embedding_model' : None,\n",
    "        'Bidirectional' : False\n",
    "    }\n",
    "    param['embedding_model'] = KeyedVectors.load_word2vec_format('./fasttext.bin', binary=True)\n",
    "    # 探索するパラメータの空間を指定する\n",
    "    param_space = {\n",
    "        'embedding_dropout': hp.quniform('embedding_dropout', 0, 0.5, 0.05),\n",
    "        'lstm_dropout' : hp.quniform('lstm_dropout', 0, 0.5, 0.05),\n",
    "        'recurrent_dropout' : hp.quniform('recurrent_dropout', 0, 0.5, 0.05),\n",
    "        'hidden_layers': hp.quniform('hidden_layers', 1, 3, 1),\n",
    "        'hidden_units': hp.quniform('hidden_units', 32, 512, 32),\n",
    "        'hidden_activation': hp.choice('hidden_activation', ['prelu', 'relu']),\n",
    "        'hidden_dropout': hp.quniform('hidden_dropout', 0, 0.3, 0.05),\n",
    "        'batch_norm': hp.choice('batch_norm', ['before_act', 'no']),\n",
    "        'optimizer': hp.choice('optimizer',\n",
    "                           [{'type': 'adam',\n",
    "                             'lr': hp.loguniform('adam_lr', np.log(0.00001), np.log(0.01))},\n",
    "                            {'type': 'sgd',\n",
    "                             'lr': hp.loguniform('sgd_lr', np.log(0.00001), np.log(0.01))}]),\n",
    "        'batch_size': hp.quniform('batch_size', 32, 128, 32),\n",
    "        #'nb_epoch': hp.quniform('nb_epoch', 200, 500, 50)\n",
    "    }\n",
    "\n",
    "    #features = [\n",
    "    #    \"bow\",\"bow_nva\",\"bow_tf-idf\",\"term_2-gram\",\"term_3-gram\",\"word2vec_mean\",\"word2vec_pre_mean\",\n",
    "    #    \"word2vec_fine-tuning\", \"doc2vec\", \"scdv\", \"bert\"\n",
    "    #]\n",
    "    features = [\n",
    "       \"raw_text\" \n",
    "    ]\n",
    "\n",
    "    for i, name in enumerate(features):\n",
    "        train_x = load_x_train(name)\n",
    "        train_y = load_y_train(name)\n",
    "        skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=71)\n",
    "        tr_idx, va_idx = list(skf.split(train_x, train_y))[0]\n",
    "        tr_x, va_x = train_x[tr_idx], train_x[va_idx]\n",
    "        tr_y, va_y = train_y[tr_idx], train_y[va_idx]\n",
    "\n",
    "        # hyperoptによるパラメータ探索の実行\n",
    "        max_evals = 100\n",
    "        trials = Trials()\n",
    "        history = []\n",
    "        fmin(objective, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "        history = sorted(history, key=lambda tpl: tpl[1])\n",
    "        best = history[0]\n",
    "        logger.info(f'{name} - best params:{best[0]}, score:{best[1]:.4f}')\n",
    "        \n",
    "        res = pd.DataFrame.from_dict(\n",
    "                best,\n",
    "                orient='index', \n",
    "                columns=[name]\n",
    "            )\n",
    "        if i == 0:\n",
    "            res.to_csv('./model/tuning/LSTM.csv')\n",
    "        else:\n",
    "            res.to_csv('./model/tuning/LSTM.csv', mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEsCOD8YONjE"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM_tuning_for_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
