{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/banboooo044/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hyperdash\n",
    "from hyperdash import monitor_cell\n",
    "!hyperdash signup --github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from hyperdash import Experiment\n",
    "\n",
    "class Hyperdash(Callback):\n",
    "    def __init__(self, entries, exp):\n",
    "        super(Hyperdash, self).__init__()\n",
    "        self.entries = entries\n",
    "        self.exp = exp\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for entry in self.entries:\n",
    "            log = logs.get(entry)            \n",
    "            if log is not None:\n",
    "                self.exp.metric(entry, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp ./drive/My\\ Drive/train-val-pre.tsv ./\n",
    "%cp ./drive/My\\ Drive/fasttext-normal.bin ./\n",
    "%cp ./drive/My\\ Drive/fasttext-neologd.bin ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir model\n",
    "%mkdir model/tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.model import Model\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from src.util import Logger, Util\n",
    "from sklearn.model_selection import learning_curve\n",
    "from scipy import sparse\n",
    "from scipy.sparse import load_npz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import load_npz\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import SpatialDropout1D, Bidirectional\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.advanced_activations import ReLU, PReLU\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class Util:\n",
    "    @classmethod\n",
    "    def dump(cls, value, path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        joblib.dump(value, path, compress=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return joblib.load(path)\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.general_logger = logging.getLogger('general')\n",
    "        self.result_logger = logging.getLogger('result')\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        file_general_handler = logging.FileHandler('./model/general.log')\n",
    "        file_result_handler = logging.FileHandler('./model/result.log')\n",
    "        if len(self.general_logger.handlers) == 0:\n",
    "            self.general_logger.addHandler(stream_handler)\n",
    "            self.general_logger.addHandler(file_general_handler)\n",
    "            self.general_logger.setLevel(logging.INFO)\n",
    "            self.result_logger.addHandler(stream_handler)\n",
    "            self.result_logger.addHandler(file_result_handler)\n",
    "            self.result_logger.setLevel(logging.INFO)\n",
    "\n",
    "    def info(self, message):\n",
    "        # 時刻をつけてコンソールとログに出力\n",
    "        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n",
    "\n",
    "    def result(self, message):\n",
    "        self.result_logger.info(message)\n",
    "\n",
    "    def result_ltsv(self, dic):\n",
    "        self.result(self.to_ltsv(dic))\n",
    "\n",
    "    def result_scores(self, run_name, scores):\n",
    "        # 計算結果をコンソールと計算結果用ログに出力\n",
    "        dic = dict()\n",
    "        dic['name'] = run_name\n",
    "        dic['score'] = np.mean(scores)\n",
    "        for i, score in enumerate(scores):\n",
    "            dic[f'score{i}'] = score\n",
    "        self.result_ltsv(dic)\n",
    "\n",
    "    def now_string(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    def to_ltsv(self, dic):\n",
    "        return '\\t'.join(['{}:{}'.format(key, value) for key, value in dic.items()])\n",
    "\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self, run_name: str, model_cls: Callable[[str, dict], Model], features: str, params: dict):\n",
    "        \"\"\"コンストラクタ\n",
    "        :param run_name: ランの名前\n",
    "        :param model_cls: モデルのクラス\n",
    "        :param features: 特徴量のリスト\n",
    "        :param params: ハイパーパラメータ\n",
    "        \"\"\"\n",
    "        self.run_name = run_name\n",
    "        self.model_cls = model_cls\n",
    "        self.features = features\n",
    "        self.params = params\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "        self.test_x = None\n",
    "        self.n_fold = 5\n",
    "\n",
    "    def train_fold(self, i_fold: Union[int, str]) -> Tuple[\n",
    "        Model, Optional[np.array], Optional[np.array], Optional[float]]:\n",
    "        \"\"\"クロスバリデーションでのfoldを指定して学習・評価を行う\n",
    "        他のメソッドから呼び出すほか、単体でも確認やパラメータ調整に用いる\n",
    "        :param i_fold: foldの番号（すべてのときには'all'とする）\n",
    "        :return: （モデルのインスタンス、レコードのインデックス、予測値、評価によるスコア）のタプル\n",
    "        \"\"\"\n",
    "        # 学習データの読込\n",
    "        validation = i_fold != 'all'\n",
    "        train_x = self.load_x_train()\n",
    "        train_y = self.load_y_train()\n",
    "        if validation:\n",
    "            # 学習データ・バリデーションデータをセットする\n",
    "            tr_idx, va_idx = self.load_index_fold(i_fold)\n",
    "            tr_x, tr_y = train_x[tr_idx], train_y[tr_idx]\n",
    "            va_x, va_y = train_x[va_idx], train_y[va_idx]\n",
    "            \n",
    "            # 学習を行う\n",
    "            model = self.build_model(i_fold)\n",
    "            model.train(tr_x, tr_y, va_x, va_y)\n",
    "\n",
    "            # バリデーションデータへの予測・評価を行う\n",
    "            va_pred = model.predict(va_x)\n",
    "            score = model.score(va_x, va_y)\n",
    "\n",
    "            # モデル、インデックス、予測値、評価を返す\n",
    "            return model, va_idx, va_pred, score\n",
    "        else:\n",
    "            # 学習データ全てで学習を行う\n",
    "            model = self.build_model(i_fold)\n",
    "            model.train(train_x, train_y)\n",
    "\n",
    "            # モデルを返す\n",
    "            return model, None, None, None\n",
    "\n",
    "    def train_fold_lr(self, i_fold: Union[int, str], lr_curve_train_sizes: List[int]) -> Tuple[\n",
    "        Model, Optional[np.array], Optional[np.array], Optional[float]]:\n",
    "        \"\"\"クロスバリデーションでのfoldを指定して学習・評価を行う\n",
    "        他のメソッドから呼び出すほか、単体でも確認やパラメータ調整に用いる\n",
    "        :param i_fold: foldの番号（すべてのときには'all'とする）\n",
    "        :return: （モデルのインスタンス、レコードのインデックス、予測値、評価によるスコア）のタプル\n",
    "        \"\"\"\n",
    "        # 学習データの読込\n",
    "        train_x = self.load_x_train()\n",
    "        train_y = self.load_y_train()\n",
    "\n",
    "        tr_idx, va_idx = self.load_index_fold(i_fold)\n",
    "        # 学習を行う\n",
    "        model = self.build_model(i_fold)\n",
    "        \n",
    "        tr_x, tr_y = train_x[tr_idx], train_y[tr_idx]\n",
    "        va_x, va_y = train_x[va_idx], train_y[va_idx]\n",
    "        \n",
    "        tr_score = np.empty(len(lr_curve_train_sizes))\n",
    "        val_score = np.empty(len(lr_curve_train_sizes))\n",
    "\n",
    "        for i, n_train_samples in enumerate(lr_curve_train_sizes):\n",
    "            model.train(tr_x[:n_train_samples], tr_y[:n_train_samples], va_x, va_y)\n",
    "            tr_score[i] = model.score(tr_x[:n_train_samples], tr_y[:n_train_samples])\n",
    "            val_score[i] = model.score(va_x, va_y)\n",
    "\n",
    "        model.train(tr_x, tr_y, va_x, va_y)\n",
    "\n",
    "        va_pred = model.predict(va_x)\n",
    "        score = model.score(va_x, va_y)\n",
    "        # モデル、インデックス、トレーニングスコア, バリデーションスコアを返す\n",
    "        return model, va_idx, va_pred, score, tr_score, val_score \n",
    "\n",
    "    def run_train_cv(self, lr_curve_train_sizes: Optional[List[int]]=None) -> None:\n",
    "        \"\"\"クロスバリデーションでの学習・評価を行う\n",
    "        学習・評価とともに、各foldのモデルの保存、スコアのログ出力についても行う\n",
    "        \"\"\"\n",
    "        logger.info(f'{self.run_name} - start training cv')\n",
    "\n",
    "        scores = []\n",
    "        va_idxes = []\n",
    "        preds = []\n",
    "\n",
    "        lr_curve = not (lr_curve_train_sizes is None)\n",
    "\n",
    "        if lr_curve:\n",
    "            train_scores = np.empty(( 0, len(lr_curve_train_sizes) ) , float)\n",
    "            valid_scores = np.empty(( 0, len(lr_curve_train_sizes) ) , float)\n",
    "        # 各foldで学習を行う\n",
    "        for i_fold in range(self.n_fold):\n",
    "            # 学習を行う\n",
    "            logger.info(f'{self.run_name} fold {i_fold} - start training')\n",
    "            if lr_curve:\n",
    "                model, va_idx, va_pred, score, tr_score, val_score  = self.train_fold_lr(i_fold, lr_curve_train_sizes)\n",
    "                train_scores = np.append(train_scores, tr_score.reshape(1, len(lr_curve_train_sizes)) ,axis=0)\n",
    "                valid_scores = np.append(valid_scores, val_score.reshape(1, len(lr_curve_train_sizes)),axis=0)\n",
    "            else:\n",
    "                model, va_idx, va_pred, score = self.train_fold(i_fold)\n",
    "            logger.info(f'{self.run_name} fold {i_fold} - end training - score {score}')\n",
    "\n",
    "            # モデルを保存する\n",
    "            model.save_model(self.features)\n",
    "\n",
    "            # 結果を保持する\n",
    "            va_idxes.append(va_idx)\n",
    "            scores.append(score)\n",
    "            preds.append(va_pred)\n",
    "\n",
    "        # 各foldの結果をまとめる\n",
    "        va_idxes = np.concatenate(va_idxes)\n",
    "        order = np.argsort(va_idxes)\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        preds = preds[order]\n",
    "\n",
    "        logger.info(f'{self.run_name} - end training cv - score {np.mean(scores)}')\n",
    "\n",
    "        # 予測結果の保存\n",
    "        Util.dump(preds, f'./model/pred/{self.features}/{self.run_name}-train.pkl')\n",
    "\n",
    "        # 評価結果の保存\n",
    "        logger.result_scores(self.run_name, scores)\n",
    "        if lr_curve:\n",
    "            self._plot_lr_curve(lr_curve_train_sizes, train_scores, valid_scores)\n",
    "            \n",
    "\n",
    "    def _plot_lr_curve(self, lr_curve_train_sizes : List[int], train_scores: np.array, valid_scores: np.array) -> None:\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "        train_mean = np.mean(train_scores, axis=0)\n",
    "        train_std  = np.std(train_scores, axis=0)\n",
    "        valid_mean = np.mean(valid_scores, axis=0)\n",
    "        valid_std  = np.std(valid_scores, axis=0)   \n",
    "        plt.plot(lr_curve_train_sizes, train_mean, color='orange', marker='o', markersize=5, label='lerning-curve')\n",
    "        plt.fill_between(lr_curve_train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.1, color='orange')\n",
    "        plt.plot(lr_curve_train_sizes, valid_mean, color='darkblue', marker='o', markersize=5,label='validation accuracy')\n",
    "        plt.fill_between(lr_curve_train_sizes, valid_mean + valid_std,valid_mean - valid_std, alpha=0.1, color='darkblue') \n",
    "        plt.xlabel('#training samples')\n",
    "        plt.ylabel('scores')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.savefig(f'./model/fig/learning-curve-{self.run_name}.png',dpi=300)\n",
    "\n",
    "\n",
    "    def run_predict_cv(self) -> None:\n",
    "        \"\"\"クロスバリデーションで学習した各foldのモデルの平均により、テストデータの予測を行う\n",
    "        あらかじめrun_train_cvを実行しておく必要がある\n",
    "        \"\"\"\n",
    "        logger.info(f'{self.run_name} - start prediction cv')\n",
    "\n",
    "        test_x = self.load_x_test()\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        # 各foldのモデルで予測を行う\n",
    "        for i_fold in range(self.n_fold):\n",
    "            logger.info(f'{self.run_name} - start prediction fold:{i_fold}')\n",
    "            model = self.build_model(i_fold)\n",
    "            model.load_model(self.features)\n",
    "            pred = model.predict(test_x)\n",
    "            preds.append(pred)\n",
    "            logger.info(f'{self.run_name} - end prediction fold:{i_fold}')\n",
    "\n",
    "        # 予測の平均値を出力する\n",
    "        pred_avg = np.mean(preds, axis=0)\n",
    "\n",
    "        # 予測結果の保存\n",
    "        Util.dump(pred_avg, f'./model/pred/{self.run_name}-test.pkl')\n",
    "\n",
    "        logger.info(f'{self.run_name} - end prediction cv')\n",
    "\n",
    "    def run_train_all(self) -> None:\n",
    "        \"\"\"学習データすべてで学習し、そのモデルを保存する\"\"\"\n",
    "        logger.info(f'{self.run_name} - start training all')\n",
    "\n",
    "        # 学習データ全てで学習を行う\n",
    "        i_fold = 'all'\n",
    "        model, _, _, _ = self.train_fold(i_fold)\n",
    "        model.save_model(self.features)\n",
    "\n",
    "        logger.info(f'{self.run_name} - end training all')\n",
    "\n",
    "    def run_predict_all(self) -> None:\n",
    "        \"\"\"学習データすべてで学習したモデルにより、テストデータの予測を行う\n",
    "        あらかじめrun_train_allを実行しておく必要がある\n",
    "        \"\"\"\n",
    "        logger.info(f'{self.run_name} - start prediction all')\n",
    "\n",
    "        test_x = self.load_x_test()\n",
    "\n",
    "        # 学習データ全てで学習したモデルで予測を行う\n",
    "        i_fold = 'all'\n",
    "        model = self.build_model(i_fold)\n",
    "        model.load_model(self.features)\n",
    "        pred = model.predict(test_x)\n",
    "\n",
    "        # 予測結果の保存\n",
    "        Util.dump(pred, f'./model/pred/{self.run_name}-test.pkl')\n",
    "\n",
    "        logger.info(f'{self.run_name} - end prediction all')\n",
    "\n",
    "    def build_model(self, i_fold: Union[int, str]) -> Model:\n",
    "        \"\"\"クロスバリデーションでのfoldを指定して、モデルの作成を行う\n",
    "        :param i_fold: foldの番号\n",
    "        :return: モデルのインスタンス\n",
    "        \"\"\"\n",
    "        # ラン名、fold、モデルのクラスからモデルを作成する\n",
    "        run_fold_name = f'{self.run_name}-{i_fold}'\n",
    "        return self.model_cls(run_fold_name, **self.params)\n",
    "\n",
    "    def load_x_train(self) -> np.array:\n",
    "        \"\"\"学習データの特徴量を読み込む\n",
    "        :return: 学習データの特徴量\n",
    "        \"\"\"\n",
    "        # 学習データの読込を行う\n",
    "        # 列名で抽出する以上のことを行う場合、このメソッドの修正が必要\n",
    "        # 毎回train.csvを読み込むのは効率が悪いため、データに応じて適宜対応するのが望ましい（他メソッドも同様）\n",
    "        if self.train_x is None:\n",
    "            if self.features == \"bow\":\n",
    "                self.train_x =load_npz('../vec/bow_train_x.npz').astype('float64')\n",
    "            elif self.features == \"n-gram\":\n",
    "                self.train_x =load_npz('../vec/n-gram_x.npz').astype('float64')\n",
    "            elif self.features == \"tf-idf\":\n",
    "                self.train_x = load_npz('../vec/tf-idf_x.npz').astype('float64')\n",
    "            elif self.features == \"n-gram-tf-idf\":\n",
    "                self.train_x =load_npz('../vec/n-gram-tf-idf_x.npz').astype('float64')\n",
    "            elif self.features == \"word2vec_mean\":\n",
    "                self.train_x = np.load('../vec/word2vec_pre_x_mean.npy', allow_pickle = True)\n",
    "            elif self.features == \"word2vec_max\":\n",
    "                self.train_x = np.load('../vec/word2vec_pre_x_max.npy', allow_pickle = True)\n",
    "            elif self.features == \"word2vec_concat\":\n",
    "                l = np.load('../vec/word2vec_pre_x_mean.npy', allow_pickle = True)\n",
    "                r = np.load('../vec/word2vec_pre_x_max.npy', allow_pickle = True)\n",
    "                self.train_x = np.hstack((l, r))\n",
    "            elif self.features == \"word2vec_hier\":\n",
    "                self.train_x = np.load('../vec/word2vec_pre_x_hier.npy', allow_pickle = True)\n",
    "            elif self.features == \"fasttext_mean\":\n",
    "                self.train_x = np.load('../vec/fasttext_x_mean.npy', allow_pickle = True)\n",
    "            elif self.features == \"fasttext_max\":\n",
    "                self.train_x = np.load('../vec/fasttext_x_max.npy', allow_pickle = True)\n",
    "            elif self.features == \"fasttext_concat\":\n",
    "                l = np.load('../vec/fasttext_x_mean.npy', allow_pickle = True)\n",
    "                r = np.load('../vec/fasttext_x_max.npy', allow_pickle = True)\n",
    "                self.train_x = np.hstack((l, r))\n",
    "            elif self.features == \"fasttext_hier\":\n",
    "                self.train_x = np.load('../vec/fasttext_x_hier.npy', allow_pickle = True)\n",
    "            elif self.features == \"doc2vec\":\n",
    "                self.train_x = np.load('../vec/doc2vec_pre_x.npy', allow_pickle=True)\n",
    "            elif self.features == \"scdv\":\n",
    "                matrix = np.load('../vec/word2vec_scdv_x.npy', allow_pickle=True)\n",
    "                self.train_x = sparse.csr_matrix(matrix, dtype=np.float64)\n",
    "            elif self.features == \"bert\":\n",
    "                self.train_x = np.load('../vec/bert_x.npy', allow_pickle=True)\n",
    "            elif self.features == \"raw_text\":\n",
    "                df = pd.read_table(\"./train-val-wakati-juman.tsv\", index_col=0)\n",
    "                self.train_x = np.array(df[\"text\"], dtype=str)\n",
    "                self.train_y = np.array(df[\"label\"], dtype=int)\n",
    "        return self.train_x\n",
    "    \n",
    "    def load_y_train(self) -> np.array:\n",
    "        \"\"\"学習データの目的変数を読み込む\n",
    "        :return: 学習データの目的変数\n",
    "        \"\"\"\n",
    "        # 目的変数の読込を行う\n",
    "        if self.train_y is None:\n",
    "            if self.features == \"bow_nva\":\n",
    "                self.train_y = np.load('../vec/bow_train_y_nva.npy', allow_pickle = True)\n",
    "            else:\n",
    "                self.train_y = np.load('../vec/y_full.npy', allow_pickle=True).astype('float64')\n",
    "                \n",
    "        return self.train_y\n",
    "\n",
    "    def load_x_test(self) -> np.array:\n",
    "        \"\"\"テストデータの特徴量を読み込む\n",
    "        :return: テストデータの特徴量\n",
    "        \"\"\"\n",
    "        if self.test_x is None:\n",
    "            if self.features == \"bow\":\n",
    "                matrix = np.load('../vec/bow_test_x.npy', allow_pickle = True)\n",
    "            elif self.features == \"bow_nva\":\n",
    "                matrix = np.load('../vec/bow_test_x_nva.npy', allow_pickle = True)   \n",
    "            self.test_x = sparse.csr_matrix(matrix, dtype=np.float64)\n",
    "        return self.test_x\n",
    "    \n",
    "    def load_index_fold(self, i_fold: int) -> np.array:\n",
    "        \"\"\"クロスバリデーションでのfoldを指定して対応するレコードのインデックスを返す\n",
    "        :param i_fold: foldの番号\n",
    "        :return: foldに対応するレコードのインデックス\n",
    "        \"\"\"\n",
    "        # 学習データ・バリデーションデータを分けるインデックスを返す\n",
    "        # ここでは乱数を固定して毎回作成しているが、ファイルに保存する方法もある\n",
    "        train_y = self.load_y_train()\n",
    "        dummy_x = np.zeros(len(train_y))\n",
    "        skf = StratifiedKFold(n_splits=self.n_fold, shuffle=True, random_state=71)\n",
    "        return list(skf.split(dummy_x, train_y))[i_fold]\n",
    "\n",
    "class Model(metaclass=ABCMeta):\n",
    "    def __init__(self, run_fold_name: str, params: dict) -> None:\n",
    "        \"\"\"コンストラクタ\n",
    "        :param run_fold_name: ランの名前とfoldの番号を組み合わせた名前\n",
    "        :param params: ハイパーパラメータ\n",
    "        \"\"\"\n",
    "        self.run_fold_name = run_fold_name\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, tr_x: np.array, tr_y: np.array,\n",
    "              va_x: Optional[np.array] = None,\n",
    "              va_y: Optional[np.array] = None) -> None:\n",
    "        \"\"\"モデルの学習を行い、学習済のモデルを保存する\n",
    "        :param tr_x: 学習データの特徴量\n",
    "        :param tr_y: 学習データの目的変数\n",
    "        :param va_x: バリデーションデータの特徴量\n",
    "        :param va_y: バリデーションデータの目的変数\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, te_x: np.array) -> np.array:\n",
    "        \"\"\"学習済のモデルでの予測値を返す\n",
    "        :param te_x: バリデーションデータやテストデータの特徴量\n",
    "        :return: 予測値\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, te_x: np.array, te_y: np.array) -> float:\n",
    "        \"\"\"学習済のモデルでのスコア値を返す\n",
    "        :te_x: np.array\n",
    "        :te_y: np.array\n",
    "        :return: 予測値\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model(self, feature: str) -> None:\n",
    "        \"\"\"モデルの保存を行う\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self, feature: str) -> None:\n",
    "        \"\"\"モデルの読み込みを行う\"\"\"\n",
    "        pass\n",
    "\n",
    "# tensorflowの警告抑制\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "class ModelLSTM(Model):\n",
    "    def __init__(self, run_fold_name, **params):\n",
    "        super().__init__(run_fold_name, params)\n",
    "\n",
    "    def train(self, tr_x, tr_y, va_x=None, va_y=None):\n",
    "        \"\"\" \n",
    "            tr_x : List[str] (example.) [ \"I am happy\", \"hello\" ]\n",
    "            tr_y : List[label]\n",
    "            embedding_model : gensim.models.KeyedVectors Object\n",
    "        \"\"\"\n",
    "        exp = Experiment(\"LSTM-Twitter\")\n",
    "        hd_callback = Hyperdash([\"val_loss\", \"loss\", \"val_accuracy\", \"accuracy\"], exp)\n",
    "        # scaling\n",
    "        validation = va_x is not None\n",
    "\n",
    "        # パラメータ\n",
    "        nb_classes = 5\n",
    "        embedding_dropout = self.params['embedding_dropout']\n",
    "        lstm_dropout = self.params['lstm_dropout']\n",
    "        lstm_recurrent_dropout = self.params['recurrent_dropout']\n",
    "        hidden_layers = int(self.params['hidden_layers'])\n",
    "        hidden_units = int(self.params['hidden_units'])\n",
    "        hidden_activation = self.params['hidden_activation']\n",
    "        hidden_dropout = self.params['hidden_dropout']\n",
    "        batch_norm = self.params['batch_norm']\n",
    "        optimizer_type = self.params['optimizer']['type']\n",
    "        optimizer_lr = self.params['optimizer']['lr']\n",
    "        batch_size = int(self.params['batch_size'])\n",
    "        nb_epoch = int(self.params['nb_epoch'])\n",
    "        embedding_model = self.params['embedding_model']\n",
    "        bidirectional = self.params['Bidirectional']\n",
    "        use_pre_embedding = not (embedding_model is None)\n",
    "\n",
    "        # using keras tokenizer here\n",
    "        self.token = Tokenizer(num_words=None)\n",
    "        self.max_len = 70\n",
    "        if validation:\n",
    "            self.token.fit_on_texts(list(tr_x) + list(va_x))\n",
    "        else:\n",
    "            self.token.fit_on_texts(list(tr_x))\n",
    "\n",
    "        xtrain_seq = self.token.texts_to_sequences(tr_x)\n",
    "        tr_x = pad_sequences(xtrain_seq, maxlen=self.max_len)\n",
    "        tr_y = np_utils.to_categorical(tr_y, num_classes=nb_classes)\n",
    "\n",
    "        if validation:\n",
    "            xvalid_seq = self.token.texts_to_sequences(va_x)\n",
    "            va_x = pad_sequences(xvalid_seq, maxlen=self.max_len)\n",
    "            va_y = np_utils.to_categorical(va_y, num_classes=nb_classes)\n",
    "\n",
    "        word_index = self.token.word_index\n",
    "\n",
    "        if use_pre_embedding:\n",
    "            # create an embedding matrix\n",
    "            vector_dim = embedding_model.vector_size\n",
    "            embedding_matrix = np.zeros((len(word_index) + 1, vector_dim))\n",
    "            for word, i in tqdm(word_index.items()):\n",
    "                embedding_vector = embedding_model.wv[word]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "        self.model = Sequential()\n",
    "        # input layer\n",
    "        if use_pre_embedding:\n",
    "            self.model.add(Embedding(\n",
    "                    input_dim=len(word_index) + 1, \n",
    "                    output_dim=vector_dim,\n",
    "                    input_length=self.max_len,\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False))\n",
    "        else:\n",
    "            self.model.add(Embedding(input_dim=len(word_index) + 1, \n",
    "                    output_dim=300,\n",
    "                    input_length=self.max_len))\n",
    "\n",
    "        self.model.add(SpatialDropout1D(embedding_dropout))\n",
    "        if bidirectional:\n",
    "            self.model.add(Bidirectional(LSTM(300, dropout=lstm_dropout, recurrent_dropout=lstm_recurrent_dropout)))\n",
    "        else:\n",
    "            self.model.add(LSTM(100, dropout=lstm_dropout, recurrent_dropout=lstm_recurrent_dropout))\n",
    "        # 中間層\n",
    "        for i in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_units))\n",
    "            if batch_norm == 'before_act':\n",
    "                self.model.add(BatchNormalization())\n",
    "            if hidden_activation == 'prelu':\n",
    "                self.model.add(PReLU())\n",
    "            elif hidden_activation == 'relu':\n",
    "                self.model.add(ReLU())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.model.add(Dropout(hidden_dropout))\n",
    "\n",
    "        # 出力層\n",
    "        self.model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "        # オプティマイザ\n",
    "        if optimizer_type == 'sgd':\n",
    "            optimizer = SGD(lr=optimizer_lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        elif optimizer_type == 'adam':\n",
    "            optimizer = Adam(lr=optimizer_lr, beta_1=0.9, beta_2=0.999, decay=0.)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # 目的関数、評価指標などの設定\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # エポック数、アーリーストッピング\n",
    "        # あまりepochを大きくすると、小さい学習率のときに終わらないことがあるので注意\n",
    "        patience = 12\n",
    "        # 学習の実行\n",
    "        if validation:\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience,\n",
    "                                            verbose=1, restore_best_weights=True)\n",
    "            history = self.model.fit(tr_x, tr_y, epochs=nb_epoch, batch_size=batch_size, verbose=2,\n",
    "                                validation_data=(va_x, va_y), callbacks=[early_stopping, hd_callback])\n",
    "        else:\n",
    "            history = self.model.fit(tr_x, tr_y, nb_epoch=nb_epoch, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    def predict(self, te_x):\n",
    "        xtest_seq = self.token.texts_to_sequences(te_x)\n",
    "        te_x = pad_sequences(xtest_seq, maxlen=self.max_len)\n",
    "        y_pred = self.model.predict(te_x)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, te_x, te_y):\n",
    "        y_pred = self.predict(te_x)\n",
    "        return f1_score(te_y, np.argmax(y_pred, axis=1), average='samples')\n",
    "\n",
    "    def save_model(self, feature):\n",
    "        model_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}.h5')\n",
    "        scaler_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}-scaler.pkl')\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        self.model.save(model_path)\n",
    "\n",
    "    def load_model(self, feature):\n",
    "        model_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}.h5')\n",
    "        scaler_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}-scaler.pkl')\n",
    "        self.model = load_model(model_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-10 11:28:14] - LSTM1 - start training cv\n",
      "[2019-12-10 11:28:14] - LSTM1 fold 0 - start training\n",
      "  0%|          | 0/181555 [00:00<?, ?it/s]/Users/banboooo044/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/ipykernel_launcher.py:510: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "100%|██████████| 181555/181555 [00:01<00:00, 153541.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f58f33b532e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mparams_LSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LSTM1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModelLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raw_text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_LSTM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#runner.run_train_cv([ 100, 500, 1000, 2000, 5000, 6000, 7000, 8000])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#runner.run_predict_cv()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-54e15c97f132>\u001b[0m in \u001b[0;36mrun_train_cv\u001b[0;34m(self, lr_curve_train_sizes)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mvalid_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_curve_train_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{self.run_name} fold {i_fold} - end training - score {score}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-54e15c97f132>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(self, i_fold)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# 学習を行う\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mva_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# バリデーションデータへの予測・評価を行う\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-54e15c97f132>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, tr_x, tr_y, va_x, va_y)\u001b[0m\n\u001b[1;32m    566\u001b[0m                                             verbose=1, restore_best_weights=True)\n\u001b[1;32m    567\u001b[0m             history = self.model.fit(tr_x, tr_y, epochs=nb_epoch, batch_size=batch_size, verbose=2,\n\u001b[0;32m--> 568\u001b[0;31m                                 validation_data=(va_x, va_y), callbacks=[early_stopping])\n\u001b[0m\u001b[1;32m    569\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.3.1/envs/ml3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "        'embedding_dropout' : 0.3,\n",
    "        'lstm_dropout' : 0.3,\n",
    "        'recurrent_dropout' : 0.3,\n",
    "        'hidden_layers': 3,\n",
    "        'hidden_units': 1024,\n",
    "        'hidden_activation': 'relu',\n",
    "        'hidden_dropout': 0.8,\n",
    "        'batch_norm': 'before_act',\n",
    "        'optimizer': {'type': 'adam', 'lr': 0.001},\n",
    "        'batch_size': 500,\n",
    "        'nb_epoch' : 10,\n",
    "        'embedding_model' : None,\n",
    "        'Bidirectional' : False\n",
    "    }\n",
    "params['embedding_model'] = KeyedVectors.load_word2vec_format('./fasttext-neologd.bin', binary=True)\n",
    "#params['embedding_model'] = KeyedVectors.load_word2vec_format('./fasttext-normal.bin', binary=True)\n",
    "params_LSTM = dict(params)\n",
    "\n",
    "runner = Runner(run_name='LSTM', model_cls=ModelLSTM, features=\"raw_text\", params=params_LSTM)\n",
    "runner.train_fold_lr(0)\n",
    "\n",
    "#runner.run_train_cv([ 100, 500, 1000, 2000, 5000, 6000, 7000, 8000])\n",
    "#runner.run_predict_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
